"""
å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ - è‡ªåŠ¨æ›´æ–°äº¤æ˜“æ•°æ®
"""
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.triggers.cron import CronTrigger
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta, timezone
from zoneinfo import ZoneInfo
import os
import time
import threading
from functools import partial
from dotenv import load_dotenv
import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.trade_processor import TradeDataProcessor
from app.database import Database
from app.jobs.noon_loss_job import run_noon_loss_check, run_noon_loss_review
from app.jobs.risk_jobs import run_long_held_positions_check, run_sleep_risk_check
from app.jobs.sync_jobs import run_sync_open_positions, run_sync_trades_incremental
from app.logger import logger
from app.notifier import send_server_chan_notification
from app.binance_client import BinanceFuturesRestClient

load_dotenv()

# å®šä¹‰UTC+8æ—¶åŒº
UTC8 = ZoneInfo("Asia/Shanghai")


class TradeDataScheduler:
    """äº¤æ˜“æ•°æ®å®šæ—¶æ›´æ–°è°ƒåº¦å™¨"""

    def __init__(self):
        def _env_int(name: str, default: int, minimum: int | None = None) -> int:
            raw = os.getenv(name)
            if raw is None:
                value = default
            else:
                try:
                    value = int(raw)
                except ValueError:
                    logger.warning(f"ç¯å¢ƒå˜é‡ {name}={raw} éæ³•ï¼Œä½¿ç”¨é»˜è®¤å€¼ {default}")
                    value = default
            if minimum is not None:
                value = max(minimum, value)
            return value

        def _env_float(name: str, default: float, minimum: float | None = None) -> float:
            raw = os.getenv(name)
            if raw is None:
                value = default
            else:
                try:
                    value = float(raw)
                except ValueError:
                    logger.warning(f"ç¯å¢ƒå˜é‡ {name}={raw} éæ³•ï¼Œä½¿ç”¨é»˜è®¤å€¼ {default}")
                    value = default
            if minimum is not None:
                value = max(minimum, value)
            return value

        scheduler_tz = os.getenv('SCHEDULER_TIMEZONE', 'Asia/Shanghai')
        try:
            self.scheduler = BackgroundScheduler(timezone=ZoneInfo(scheduler_tz))
        except Exception as exc:
            logger.warning(f"æ— æ•ˆçš„è°ƒåº¦å™¨æ—¶åŒº {scheduler_tz}: {exc}ï¼Œä½¿ç”¨é»˜è®¤æ—¶åŒº")
            self.scheduler = BackgroundScheduler()
        self.db = Database()

        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        api_key = os.getenv('BINANCE_API_KEY')
        api_secret = os.getenv('BINANCE_API_SECRET')

        if not api_key or not api_secret:
            logger.warning("æœªé…ç½®Binance APIå¯†é’¥ï¼Œå®šæ—¶ä»»åŠ¡å°†æ— æ³•è¿è¡Œ")
            self.processor = None
        else:
            self.processor = TradeDataProcessor(api_key, api_secret)

        self.days_to_fetch = _env_int('DAYS_TO_FETCH', 30, minimum=1)
        self.update_interval_minutes = _env_int('UPDATE_INTERVAL_MINUTES', 10, minimum=1)
        self.open_positions_update_interval_minutes = _env_int(
            'OPEN_POSITIONS_UPDATE_INTERVAL_MINUTES',
            self.update_interval_minutes,
            minimum=1
        )
        self.start_date = os.getenv('START_DATE')  # è‡ªå®šä¹‰èµ·å§‹æ—¥æœŸ
        self.end_date = os.getenv('END_DATE')      # è‡ªå®šä¹‰ç»“æŸæ—¥æœŸ
        self.sync_lookback_minutes = _env_int('SYNC_LOOKBACK_MINUTES', 1440, minimum=1)
        self.symbol_sync_overlap_minutes = _env_int('SYMBOL_SYNC_OVERLAP_MINUTES', 1440, minimum=1)
        self.open_positions_lookback_days = _env_int('OPEN_POSITIONS_LOOKBACK_DAYS', 60, minimum=1)
        self.enable_daily_full_sync = os.getenv('ENABLE_DAILY_FULL_SYNC', '1').lower() in ('1', 'true', 'yes')
        self.daily_full_sync_hour = _env_int('DAILY_FULL_SYNC_HOUR', 3, minimum=0)
        self.daily_full_sync_minute = _env_int('DAILY_FULL_SYNC_MINUTE', 30, minimum=0)
        self.use_time_filter = os.getenv('SYNC_USE_TIME_FILTER', '1').lower() in ('1', 'true', 'yes')
        self.enable_user_stream = os.getenv('ENABLE_USER_STREAM', '0').lower() in ('1', 'true', 'yes')
        self.force_full_sync = os.getenv('FORCE_FULL_SYNC', '0').lower() in ('1', 'true', 'yes')
        self.enable_leaderboard_alert = os.getenv('ENABLE_LEADERBOARD_ALERT', '1').lower() in ('1', 'true', 'yes')
        self.leaderboard_top_n = _env_int('LEADERBOARD_TOP_N', 10, minimum=1)
        self.leaderboard_min_quote_volume = _env_float('LEADERBOARD_MIN_QUOTE_VOLUME', 50_000_000, minimum=0.0)
        self.leaderboard_max_symbols = _env_int('LEADERBOARD_MAX_SYMBOLS', 120, minimum=0)
        self.leaderboard_kline_workers = _env_int('LEADERBOARD_KLINE_WORKERS', 6, minimum=1)
        self.leaderboard_weight_budget_per_minute = _env_int('LEADERBOARD_WEIGHT_BUDGET_PER_MINUTE', 900, minimum=60)
        self.leaderboard_alert_hour = _env_int('LEADERBOARD_ALERT_HOUR', 7, minimum=0)
        self.leaderboard_alert_minute = _env_int('LEADERBOARD_ALERT_MINUTE', 40, minimum=0)
        self.leaderboard_guard_before_minutes = _env_int('LEADERBOARD_GUARD_BEFORE_MINUTES', 2, minimum=0)
        self.leaderboard_guard_after_minutes = _env_int('LEADERBOARD_GUARD_AFTER_MINUTES', 5, minimum=0)
        self.enable_rebound_7d_snapshot = os.getenv('ENABLE_REBOUND_7D_SNAPSHOT', '1').lower() in ('1', 'true', 'yes')
        self.rebound_7d_top_n = _env_int('REBOUND_7D_TOP_N', 10, minimum=1)
        self.rebound_7d_kline_workers = _env_int('REBOUND_7D_KLINE_WORKERS', 6, minimum=1)
        self.rebound_7d_weight_budget_per_minute = _env_int('REBOUND_7D_WEIGHT_BUDGET_PER_MINUTE', 900, minimum=60)
        self.rebound_7d_hour = _env_int('REBOUND_7D_HOUR', 7, minimum=0)
        self.rebound_7d_minute = _env_int('REBOUND_7D_MINUTE', 30, minimum=0)
        self.enable_rebound_30d_snapshot = os.getenv('ENABLE_REBOUND_30D_SNAPSHOT', '1').lower() in ('1', 'true', 'yes')
        self.rebound_30d_top_n = _env_int('REBOUND_30D_TOP_N', 10, minimum=1)
        self.rebound_30d_kline_workers = _env_int('REBOUND_30D_KLINE_WORKERS', self.rebound_7d_kline_workers, minimum=1)
        self.rebound_30d_weight_budget_per_minute = _env_int(
            'REBOUND_30D_WEIGHT_BUDGET_PER_MINUTE', self.rebound_7d_weight_budget_per_minute, minimum=60
        )
        self.rebound_30d_hour = _env_int('REBOUND_30D_HOUR', self.rebound_7d_hour, minimum=0)
        self.rebound_30d_minute = _env_int('REBOUND_30D_MINUTE', self.rebound_7d_minute + 2, minimum=0)
        self.enable_rebound_60d_snapshot = os.getenv('ENABLE_REBOUND_60D_SNAPSHOT', '1').lower() in ('1', 'true', 'yes')
        self.rebound_60d_top_n = _env_int('REBOUND_60D_TOP_N', 10, minimum=1)
        self.rebound_60d_kline_workers = _env_int('REBOUND_60D_KLINE_WORKERS', self.rebound_7d_kline_workers, minimum=1)
        self.rebound_60d_weight_budget_per_minute = _env_int(
            'REBOUND_60D_WEIGHT_BUDGET_PER_MINUTE', self.rebound_7d_weight_budget_per_minute, minimum=60
        )
        self.rebound_60d_hour = _env_int('REBOUND_60D_HOUR', self.rebound_7d_hour, minimum=0)
        self.rebound_60d_minute = _env_int('REBOUND_60D_MINUTE', self.rebound_7d_minute + 4, minimum=0)
        self.noon_loss_check_hour = _env_int('NOON_LOSS_CHECK_HOUR', 11, minimum=0)
        self.noon_loss_check_minute = _env_int('NOON_LOSS_CHECK_MINUTE', 50, minimum=0)
        self.noon_review_hour = _env_int('NOON_REVIEW_HOUR', 23, minimum=0)
        self.noon_review_minute = _env_int('NOON_REVIEW_MINUTE', 2, minimum=0)
        self.noon_review_target_day_offset = _env_int('NOON_REVIEW_TARGET_DAY_OFFSET', 0)
        self.enable_profit_alert = os.getenv('ENABLE_PROFIT_ALERT', '1').lower() in ('1', 'true', 'yes')
        self.enable_reentry_alert = os.getenv('ENABLE_REENTRY_ALERT', '1').lower() in ('1', 'true', 'yes')
        self.profit_alert_threshold_pct = _env_float('PROFIT_ALERT_THRESHOLD_PCT', 20.0, minimum=0.0)
        self.leaderboard_alert_hour %= 24
        self.leaderboard_alert_minute %= 60
        self.rebound_7d_hour %= 24
        self.rebound_7d_minute %= 60
        self.rebound_30d_hour %= 24
        self.rebound_30d_minute %= 60
        self.rebound_60d_hour %= 24
        self.rebound_60d_minute %= 60
        self.noon_loss_check_hour %= 24
        self.noon_loss_check_minute %= 60
        self.noon_review_hour %= 24
        self.noon_review_minute %= 60
        self.api_job_lock_wait_seconds = _env_int('API_JOB_LOCK_WAIT_SECONDS', 8, minimum=0)
        self._api_job_lock = threading.Lock()

    def _is_api_cooldown_active(self, source: str) -> bool:
        remaining = BinanceFuturesRestClient.cooldown_remaining_seconds()
        if remaining > 0:
            logger.warning(
                f"Binance APIå†·å´ä¸­ï¼Œè·³è¿‡{source}: remaining={remaining:.1f}s"
            )
            return True
        return False

    def _try_enter_api_job_slot(self, source: str) -> bool:
        wait_seconds = self.api_job_lock_wait_seconds
        if wait_seconds <= 0:
            return True

        acquired = self._api_job_lock.acquire(timeout=wait_seconds)

        if not acquired:
            logger.warning(
                f"{source}è·³è¿‡: APIä»»åŠ¡äº’æ–¥é”ç¹å¿™(ç­‰å¾…{wait_seconds}såè¶…æ—¶)"
            )
            return False
        return True

    def _release_api_job_slot(self):
        if self.api_job_lock_wait_seconds <= 0:
            return
        if self._api_job_lock.locked():
            self._api_job_lock.release()

    def _format_ms_to_utc8(self, ts_ms: int) -> str:
        """å°†æ¯«ç§’æ—¶é—´æˆ³æ ¼å¼åŒ–ä¸º UTC+8 å¯è¯»æ—¶é—´ã€‚"""
        try:
            dt = datetime.fromtimestamp(int(ts_ms) / 1000, tz=UTC8)
            return dt.strftime("%Y-%m-%d %H:%M:%S%z")
        except Exception:
            return str(ts_ms)

    def _format_window_with_ms(self, start_ms: int, end_ms: int) -> str:
        """è¾“å‡ºçª—å£çš„å¯è¯»æ—¶é—´å’ŒåŸå§‹æ¯«ç§’ï¼Œä¾¿äºæ—¥å¿—æ’æŸ¥ã€‚"""
        start_text = self._format_ms_to_utc8(start_ms)
        end_text = self._format_ms_to_utc8(end_ms)
        return f"[{start_text} ~ {end_text}] ({start_ms} ~ {end_ms})"

    def _is_leaderboard_guard_window(self) -> bool:
        """
        åœ¨æ™¨é—´æ¶¨å¹…æ¦œå‰åçŸ­æ—¶é—´çª—å£å†…è·³è¿‡äº¤æ˜“åŒæ­¥ï¼Œé¿å… API æƒé‡å åŠ ã€‚
        é»˜è®¤çª—å£: æ¦œå•å‰2åˆ†é’Ÿåˆ°å5åˆ†é’Ÿã€‚
        """
        if not self.enable_leaderboard_alert:
            return False

        now = datetime.now(UTC8)
        leaderboard_dt = now.replace(
            hour=self.leaderboard_alert_hour,
            minute=self.leaderboard_alert_minute,
            second=0,
            microsecond=0
        )
        window_start = leaderboard_dt - timedelta(minutes=self.leaderboard_guard_before_minutes)
        window_end = leaderboard_dt + timedelta(minutes=self.leaderboard_guard_after_minutes)
        return window_start <= now <= window_end

    def sync_trades_data(self, force_full: bool = False):
        """åŒæ­¥äº¤æ˜“æ•°æ®åˆ°æ•°æ®åº“"""
        if not self.processor:
            logger.warning("æ— æ³•åŒæ­¥: APIå¯†é’¥æœªé…ç½®")
            return
        if self._is_leaderboard_guard_window():
            logger.info(
                "è·³è¿‡äº¤æ˜“åŒæ­¥: ä½äºæ™¨é—´æ¶¨å¹…æ¦œä¿æŠ¤çª—å£å†… "
                f"({self.leaderboard_alert_hour:02d}:{self.leaderboard_alert_minute:02d} "
                f"å‰{self.leaderboard_guard_before_minutes}åˆ†é’Ÿè‡³å{self.leaderboard_guard_after_minutes}åˆ†é’Ÿ)"
            )
            return
        if self._is_api_cooldown_active(source='äº¤æ˜“åŒæ­¥'):
            return
        if not self._try_enter_api_job_slot(source='äº¤æ˜“åŒæ­¥'):
            return

        sync_started_at = time.perf_counter()
        symbols_elapsed = 0.0
        analyze_elapsed = 0.0
        save_trades_elapsed = 0.0
        open_positions_elapsed = 0.0
        risk_check_elapsed = 0.0
        trades_saved = 0
        open_saved = 0
        symbol_count = 0

        try:
            logger.info("=" * 50)
            run_mode = "å…¨é‡" if force_full else "å¢é‡"
            logger.info(f"å¼€å§‹åŒæ­¥äº¤æ˜“æ•°æ®... mode={run_mode}")

            # æ›´æ–°åŒæ­¥çŠ¶æ€ä¸ºè¿›è¡Œä¸­
            self.db.update_sync_status(status='syncing')

            # è·å–æœ€åä¸€æ¡äº¤æ˜“æ—¶é—´ï¼ˆä»…ä½œå‚è€ƒï¼Œä¸å†ç”¨äºå¢é‡æ›´æ–°ï¼‰
            # last_entry_time = self.db.get_last_entry_time()

            # åŒæ­¥æ¨¡å¼ï¼š
            # 1) force_full=True -> å…¨é‡æ¨¡å¼ï¼ˆæ”¯æŒ START_DATEï¼‰
            # 2) force_full=False -> å¢é‡æ¨¡å¼ï¼ˆæŒ‰æœ€åå…¥åœºæ—¶é—´å›çœ‹ï¼‰
            last_entry_time = self.db.get_last_entry_time()
            is_full_sync_run = force_full
            if force_full:
                is_full_sync_run = True
                if self.start_date:
                    try:
                        start_dt = datetime.strptime(self.start_date, '%Y-%m-%d').replace(tzinfo=UTC8)
                        start_dt = start_dt.replace(hour=23, minute=0, second=0, microsecond=0)
                        since = int(start_dt.timestamp() * 1000)
                        logger.info(f"å…¨é‡æ›´æ–°æ¨¡å¼(FORCE_FULL_SYNC) - ä»è‡ªå®šä¹‰æ—¥æœŸ {self.start_date} å¼€å§‹")
                    except ValueError as e:
                        logger.error(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤DAYS_TO_FETCH")
                        since = int((datetime.now(UTC8) - timedelta(days=self.days_to_fetch)).timestamp() * 1000)
                else:
                    logger.warning("FORCE_FULL_SYNC=1 ä½†æœªè®¾ç½® START_DATEï¼Œå›é€€ä¸º DAYS_TO_FETCH çª—å£")
                    since = int((datetime.now(UTC8) - timedelta(days=self.days_to_fetch)).timestamp() * 1000)
            elif last_entry_time:
                try:
                    last_dt = datetime.strptime(last_entry_time, '%Y-%m-%d %H:%M:%S').replace(tzinfo=UTC8)
                    since = int((last_dt - timedelta(minutes=self.sync_lookback_minutes)).timestamp() * 1000)
                    logger.info(
                        f"å¢é‡æ›´æ–°æ¨¡å¼ - ä»æœ€è¿‘å…¥åœºæ—¶é—´ {last_entry_time} å›æº¯ {self.sync_lookback_minutes} åˆ†é’Ÿ"
                    )
                except ValueError as e:
                    logger.error(f"å…¥åœºæ—¶é—´è§£æå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤DAYS_TO_FETCH")
                    since = int((datetime.now(UTC8) - timedelta(days=self.days_to_fetch)).timestamp() * 1000)
            else:
                # å¢é‡æ¨¡å¼å†·å¯åŠ¨ï¼šä½¿ç”¨ DAYS_TO_FETCH çª—å£
                logger.info(f"å¢é‡å†·å¯åŠ¨ - è·å–æœ€è¿‘ {self.days_to_fetch} å¤©æ•°æ®")
                since = int((datetime.now(UTC8) - timedelta(days=self.days_to_fetch)).timestamp() * 1000)

            # è®¡ç®—ç»“æŸæ—¶é—´
            if self.end_date:
                try:
                    end_dt = datetime.strptime(self.end_date, '%Y-%m-%d').replace(tzinfo=UTC8)
                    end_dt = end_dt.replace(hour=23, minute=59, second=59, microsecond=999000)
                    until = int(end_dt.timestamp() * 1000)
                    logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰ç»“æŸæ—¥æœŸ: {self.end_date}")
                except ValueError:
                    until = int(datetime.now(UTC8).timestamp() * 1000)
            else:
                until = int(datetime.now(UTC8).timestamp() * 1000)

            # ä»Binanceè·å–æ•°æ®
            logger.info("ä»Binance APIæŠ“å–æ•°æ®...")
            stage_started = time.perf_counter()
            traded_symbols = self.processor.get_traded_symbols(since, until)
            symbols_elapsed = time.perf_counter() - stage_started
            symbol_count = len(traded_symbols)
            logger.info(f"æ‹‰å–æ´»è·ƒäº¤æ˜“å¸ç§å®Œæˆ: count={symbol_count}, elapsed={symbols_elapsed:.2f}s")

            stage_started = time.perf_counter()
            symbol_since_map = None
            if not is_full_sync_run and traded_symbols:
                watermarks = self.db.get_symbol_sync_watermarks(traded_symbols)
                overlap_ms = self.symbol_sync_overlap_minutes * 60 * 1000
                symbol_since_map = {}
                warmed_symbols = 0
                for symbol in traded_symbols:
                    symbol_watermark = watermarks.get(symbol)
                    if symbol_watermark is None:
                        symbol_since_map[symbol] = since
                    else:
                        symbol_since_map[symbol] = max(since, symbol_watermark - overlap_ms)
                        warmed_symbols += 1
                logger.info(
                    "å¢é‡æ°´ä½ç­–ç•¥: "
                    f"symbols={len(traded_symbols)}, "
                    f"warm={warmed_symbols}, "
                    f"cold={len(traded_symbols) - warmed_symbols}, "
                    f"overlap_minutes={self.symbol_sync_overlap_minutes}"
                )

            analysis_result = self.processor.analyze_orders(
                since=since,
                until=until,
                traded_symbols=traded_symbols,
                use_time_filter=self.use_time_filter,
                symbol_since_map=symbol_since_map,
                return_symbol_status=True,
            )
            df, success_symbols, failure_symbols = analysis_result
            analyze_elapsed = time.perf_counter() - stage_started
            logger.info(f"é—­ä»“ETLå®Œæˆ: rows={len(df)}, elapsed={analyze_elapsed:.2f}s")

            if df.empty:
                logger.info("æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ›´æ–°")
            else:
                # ä»…â€œæœ¬è½®æ˜ç¡®å…¨é‡â€æ—¶æ‰å…è®¸è¦†ç›–å†™å…¥ã€‚
                # å¢é‡åŒæ­¥å¿…é¡» append/upsertï¼Œé¿å…é‡å¤åˆ å†™å¯¼è‡´æ—¥ç»Ÿè®¡æŠ–åŠ¨ã€‚
                is_full_sync = force_full

                logger.info(f"ä¿å­˜ {len(df)} æ¡è®°å½•åˆ°æ•°æ®åº“ (è¦†ç›–æ¨¡å¼={is_full_sync})...")
                stage_started = time.perf_counter()
                saved_count = self.db.save_trades(df, overwrite=is_full_sync)
                save_trades_elapsed += time.perf_counter() - stage_started
                trades_saved = saved_count

                if saved_count > 0:
                    logger.info("æ£€æµ‹åˆ°æ–°å¹³ä»“å•ï¼Œé‡ç®—ç»Ÿè®¡å¿«ç…§...")
                    stage_started = time.perf_counter()
                    self.db.recompute_trade_summary()
                    save_trades_elapsed += time.perf_counter() - stage_started

            if success_symbols:
                stage_started = time.perf_counter()
                for symbol in success_symbols:
                    self.db.update_symbol_sync_success(symbol=symbol, end_ms=until)
                save_trades_elapsed += time.perf_counter() - stage_started
                logger.info(f"åŒæ­¥æ°´ä½æ¨è¿›: success_symbols={len(success_symbols)}")
            if failure_symbols:
                stage_started = time.perf_counter()
                for symbol, err in failure_symbols.items():
                    self.db.update_symbol_sync_failure(symbol=symbol, end_ms=until, error_message=err)
                save_trades_elapsed += time.perf_counter() - stage_started
                logger.warning(f"åŒæ­¥æ°´ä½æœªæ¨è¿›(å¤±è´¥): failed_symbols={len(failure_symbols)}")

            # æ£€æŸ¥æŒä»“è¶…æ—¶å‘Šè­¦
            stage_started = time.perf_counter()
            self.check_long_held_positions()
            risk_check_elapsed = time.perf_counter() - stage_started

            # æ›´æ–°åŒæ­¥çŠ¶æ€
            self.db.update_sync_status(status='idle')

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = self.db.get_statistics()
            logger.info("åŒæ­¥å®Œæˆ!")
            logger.info(f"æ•°æ®åº“ç»Ÿè®¡: æ€»äº¤æ˜“æ•°={stats['total_trades']}, å¸ç§æ•°={stats['unique_symbols']}")
            logger.info(f"æ—¶é—´èŒƒå›´: {stats['earliest_trade']} ~ {stats['latest_trade']}")
            total_elapsed = time.perf_counter() - sync_started_at
            logger.info(
                "åŒæ­¥è€—æ—¶æ±‡æ€»: "
                f"symbols={symbols_elapsed:.2f}s, "
                f"analyze={analyze_elapsed:.2f}s, "
                f"save={save_trades_elapsed:.2f}s, "
                f"open_positions={open_positions_elapsed:.2f}s, "
                f"risk_check={risk_check_elapsed:.2f}s, "
                f"total={total_elapsed:.2f}s, "
                f"symbol_count={symbol_count}, "
                f"trades_saved={trades_saved}, "
                f"open_saved={open_saved}"
            )
            self.db.log_sync_run(
                run_type='trades_sync',
                mode='full' if force_full else 'incremental',
                status='success',
                symbol_count=symbol_count,
                rows_count=len(df),
                trades_saved=trades_saved,
                open_saved=open_saved,
                elapsed_ms=int(total_elapsed * 1000),
            )
            logger.info("=" * 50)

        except Exception as e:
            error_msg = f"åŒæ­¥å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            total_elapsed = time.perf_counter() - sync_started_at
            logger.error(
                "åŒæ­¥å¤±è´¥è€—æ—¶æ±‡æ€»: "
                f"symbols={symbols_elapsed:.2f}s, "
                f"analyze={analyze_elapsed:.2f}s, "
                f"save={save_trades_elapsed:.2f}s, "
                f"open_positions={open_positions_elapsed:.2f}s, "
                f"risk_check={risk_check_elapsed:.2f}s, "
                f"total={total_elapsed:.2f}s"
            )
            self.db.update_sync_status(status='error', error_message=error_msg)
            self.db.log_sync_run(
                run_type='trades_sync',
                mode='full' if force_full else 'incremental',
                status='error',
                symbol_count=symbol_count,
                rows_count=0,
                trades_saved=trades_saved,
                open_saved=open_saved,
                elapsed_ms=int(total_elapsed * 1000),
                error_message=error_msg,
            )
            import traceback
            logger.error(traceback.format_exc())
        finally:
            self._release_api_job_slot()

    def sync_open_positions_data(self):
        return run_sync_open_positions(self)

    def sync_trades_incremental(self):
        """å¢é‡åŒæ­¥äº¤æ˜“æ•°æ®"""
        self.sync_trades_data(force_full=False)

    def sync_trades_full(self):
        """å…¨é‡åŒæ­¥äº¤æ˜“æ•°æ®"""
        self.sync_trades_data(force_full=True)

    def _get_mark_price_map(self, symbols: list[str]) -> dict[str, float]:
        """æ‰¹é‡è·å–æ ‡è®°ä»·æ ¼ï¼ˆä¼˜å…ˆ premiumIndexï¼Œå…¶æ¬¡ ticker/priceï¼‰ã€‚"""
        if not symbols:
            return {}

        unique_symbols = sorted(set(symbols))
        resolved: dict[str, float] = {}
        missing = set(unique_symbols)

        try:
            data = self.processor.client.public_get("/fapi/v1/premiumIndex")
            if isinstance(data, dict):
                data = [data]
            for item in data or []:
                if not isinstance(item, dict):
                    continue
                symbol = str(item.get("symbol", "")).upper()
                raw_price = item.get("markPrice")
                if not symbol or raw_price is None:
                    continue
                try:
                    price = float(raw_price)
                except (TypeError, ValueError):
                    continue
                if price <= 0:
                    continue
                if symbol in missing:
                    resolved[symbol] = price
                    missing.discard(symbol)
        except Exception as exc:
            logger.warning(f"è·å–æ ‡è®°ä»·æ ¼(premiumIndex)å¤±è´¥: {exc}")

        if missing:
            try:
                data = self.processor.client.public_get("/fapi/v1/ticker/price")
                if isinstance(data, dict):
                    data = [data]
                for item in data or []:
                    if not isinstance(item, dict):
                        continue
                    symbol = str(item.get("symbol", "")).upper()
                    raw_price = item.get("price")
                    if symbol not in missing or raw_price is None:
                        continue
                    try:
                        price = float(raw_price)
                    except (TypeError, ValueError):
                        continue
                    if price <= 0:
                        continue
                    resolved[symbol] = price
                    missing.discard(symbol)
            except Exception as exc:
                logger.warning(f"è·å–æ ‡è®°ä»·æ ¼(ticker/price)å¤±è´¥: {exc}")

        if missing:
            logger.warning(
                f"ä»æœ‰{len(missing)}ä¸ªsymbolæ— æ³•è·å–å¤œé—´ä»·æ ¼: {sorted(list(missing))[:10]}"
            )

        return resolved

    @staticmethod
    def _parse_entry_time_utc8(entry_time_value) -> datetime | None:
        """è§£ææ•°æ®åº“ä¸­çš„ entry_timeï¼ˆå½“å‰æŒ‰ UTC+8 å­˜å‚¨ï¼‰ä¸º timezone-aware datetimeã€‚"""
        if not entry_time_value:
            return None
        try:
            return datetime.strptime(str(entry_time_value), '%Y-%m-%d %H:%M:%S').replace(tzinfo=UTC8)
        except ValueError:
            return None

    def check_same_symbol_reentry_alert(self):
        """åŒå¸åœ¨ UTC å½“å¤©å†…é‡å¤å¼€ä»“æé†’ï¼ˆæ¯ç¬”é‡å¤å¼€ä»“ä»…æé†’ä¸€æ¬¡ï¼‰ã€‚"""
        if not self.enable_reentry_alert:
            return

        try:
            positions = self.db.get_open_positions()
            if len(positions) < 2:
                return

            by_symbol: dict[str, list[dict]] = {}
            for pos in positions:
                symbol = str(pos.get("symbol", "")).upper().strip()
                if not symbol:
                    continue

                entry_dt_utc8 = self._parse_entry_time_utc8(pos.get("entry_time"))
                if entry_dt_utc8 is None:
                    continue

                order_id_raw = pos.get("order_id")
                try:
                    order_id = int(order_id_raw or 0)
                except (TypeError, ValueError):
                    order_id = 0

                by_symbol.setdefault(symbol, []).append({
                    "symbol": symbol,
                    "order_id": order_id,
                    "side": str(pos.get("side", "")).upper(),
                    "entry_time": str(pos.get("entry_time", "")),
                    "entry_dt_utc8": entry_dt_utc8,
                    "entry_dt_utc": entry_dt_utc8.astimezone(timezone.utc),
                    "reentry_alerted": int(pos.get("reentry_alerted", 0) or 0),
                })

            triggered = []
            for symbol, rows in by_symbol.items():
                if len(rows) < 2:
                    continue

                rows.sort(key=lambda item: (item["entry_dt_utc8"], item["order_id"]))
                for idx in range(1, len(rows)):
                    current = rows[idx]
                    previous = rows[idx - 1]

                    if current["order_id"] <= 0:
                        continue
                    if current["reentry_alerted"] == 1:
                        continue

                    if current["entry_dt_utc"].date() == previous["entry_dt_utc"].date():
                        triggered.append({
                            "symbol": symbol,
                            "side": current["side"],
                            "order_id": current["order_id"],
                            "entry_time": current["entry_time"],
                            "previous_order_id": previous["order_id"],
                            "previous_entry_time": previous["entry_time"],
                            "utc_day": current["entry_dt_utc"].strftime("%Y-%m-%d"),
                        })

            if not triggered:
                return

            triggered.sort(key=lambda item: (item["symbol"], item["entry_time"], item["order_id"]))
            title = f"âš ï¸ åŒå¸é‡å¤å¼€ä»“æé†’: {len(triggered)} ç¬”"
            content = (
                "æ£€æµ‹åˆ°ä»¥ä¸‹è®¢å•åœ¨åŒä¸€ UTC æ—¥æœŸå†…é‡å¤å¼€ä»“ï¼š\n"
                "ï¼ˆè§„åˆ™ï¼šé¦–æ¬¡å¼€ä»“åï¼ŒUTC+0 æ¬¡æ—¥ 00:00 å‰å†æ¬¡å¼€åŒå¸ï¼‰\n\n"
                "---\n"
            )

            preview_count = min(20, len(triggered))
            for item in triggered[:preview_count]:
                content += (
                    f"**{item['symbol']}** ({item['side']})\n"
                    f"- é‡å¤å¼€ä»“: #{item['order_id']} @ {item['entry_time']}\n"
                    f"- ä¸Šä¸€ç¬”: #{item['previous_order_id']} @ {item['previous_entry_time']}\n"
                    f"- UTCæ—¥æœŸ: {item['utc_day']}\n\n"
                )
            if len(triggered) > preview_count:
                content += f"... å…¶ä½™ {len(triggered) - preview_count} ç¬”æœªå±•ç¤ºã€‚\n"

            send_server_chan_notification(title, content)

            for item in triggered:
                self.db.set_position_reentry_alerted(item["symbol"], item["order_id"])

            logger.info(
                "åŒå¸é‡å¤å¼€ä»“æé†’å·²å‘é€: "
                f"count={len(triggered)}, symbols={sorted(set(item['symbol'] for item in triggered))}"
            )
        except Exception as exc:
            logger.error(f"åŒå¸é‡å¤å¼€ä»“æé†’æ£€æŸ¥å¤±è´¥: {exc}")

    def check_open_positions_profit_alert(self, threshold_pct: float):
        """æ£€æŸ¥æœªå¹³ä»“è®¢å•æµ®ç›ˆé˜ˆå€¼æé†’ï¼ˆå•æ¡£ï¼Œå•ç¬”åªæé†’ä¸€æ¬¡ï¼‰ã€‚"""
        if not self.enable_profit_alert:
            return

        try:
            positions = self.db.get_open_positions()
            if not positions:
                return

            candidates = [p for p in positions if int(p.get("profit_alerted", 0) or 0) == 0]
            if not candidates:
                return

            symbols_full = [self._normalize_futures_symbol(p.get("symbol")) for p in candidates if p.get("symbol")]
            mark_prices = self._get_mark_price_map(symbols_full)
            if not mark_prices:
                logger.warning("ç›ˆåˆ©æé†’æ£€æŸ¥è·³è¿‡: æ— æ³•è·å–æ ‡è®°ä»·æ ¼")
                return

            triggered = []
            for pos in candidates:
                symbol = str(pos.get("symbol", "")).upper()
                side = str(pos.get("side", "")).upper()
                qty = float(pos.get("qty", 0.0) or 0.0)
                entry_price = float(pos.get("entry_price", 0.0) or 0.0)
                entry_amount = float(pos.get("entry_amount", 0.0) or 0.0)
                order_id = int(pos.get("order_id", 0) or 0)
                entry_time = str(pos.get("entry_time", ""))

                if not symbol or qty <= 0 or entry_price <= 0 or entry_amount <= 0 or order_id <= 0:
                    continue

                symbol_full = self._normalize_futures_symbol(symbol)
                mark_price = mark_prices.get(symbol_full)
                if mark_price is None:
                    continue

                if side == "SHORT":
                    unrealized_pnl = (entry_price - mark_price) * qty
                else:
                    unrealized_pnl = (mark_price - entry_price) * qty

                unrealized_pct = (unrealized_pnl / entry_amount) * 100
                if unrealized_pct >= threshold_pct:
                    triggered.append({
                        "symbol": symbol,
                        "side": side,
                        "order_id": order_id,
                        "entry_time": entry_time,
                        "entry_price": entry_price,
                        "mark_price": mark_price,
                        "unrealized_pnl": unrealized_pnl,
                        "unrealized_pct": unrealized_pct
                    })

            if not triggered:
                return

            triggered.sort(key=lambda item: item["unrealized_pct"], reverse=True)
            title = f"ğŸ¯ æµ®ç›ˆæé†’: {len(triggered)} ç¬”æŒä»“è¶…è¿‡ {threshold_pct:.0f}%"
            content = (
                f"ä»¥ä¸‹æœªå¹³ä»“è®¢å•æµ®ç›ˆå·²è¾¾åˆ°é˜ˆå€¼ **{threshold_pct:.0f}%**ï¼ˆæ¯ç¬”ä»…æé†’ä¸€æ¬¡ï¼‰:\n\n"
                "--- \n"
            )
            for item in triggered:
                content += (
                    f"**{item['symbol']}** ({item['side']})\n"
                    f"- æµ®ç›ˆ: {item['unrealized_pnl']:+.2f} U ({item['unrealized_pct']:.2f}%)\n"
                    f"- å¼€ä»“: {item['entry_price']:.6g}\n"
                    f"- ç°ä»·: {item['mark_price']:.6g}\n"
                    f"- æ—¶é—´: {item['entry_time']}\n\n"
                )
            send_server_chan_notification(title, content)

            for item in triggered:
                self.db.set_position_profit_alerted(item["symbol"], item["order_id"])

            logger.info(
                "æµ®ç›ˆæé†’å·²å‘é€: "
                f"threshold={threshold_pct:.2f}%, "
                f"count={len(triggered)}, "
                f"symbols={[item['symbol'] for item in triggered]}"
            )
        except Exception as exc:
            logger.error(f"æµ®ç›ˆæé†’æ£€æŸ¥å¤±è´¥: {exc}")

    def check_long_held_positions(self):
        return run_long_held_positions_check(self)

    def check_risk_before_sleep(self):
        return run_sleep_risk_check(self)

    def check_recent_losses_at_noon(self):
        return run_noon_loss_check(self)

    def review_noon_loss_at_night(
        self,
        snapshot_date: str | None = None,
        send_notification: bool = True
    ):
        return run_noon_loss_review(
            self,
            snapshot_date=snapshot_date,
            send_notification=send_notification,
        )

    def backfill_noon_loss_review(self, snapshot_date: str, send_notification: bool = False):
        """æ‰‹åŠ¨å›å¡«æŒ‡å®šæ—¥æœŸçš„åˆé—´æ­¢æŸå¤ç›˜ç»“æœã€‚"""
        logger.info(
            f"å¼€å§‹æ‰‹åŠ¨å›å¡«åˆé—´æ­¢æŸå¤ç›˜: snapshot_date={snapshot_date}, "
            f"send_notification={send_notification}"
        )
        self.review_noon_loss_at_night(snapshot_date=snapshot_date, send_notification=send_notification)

    def _build_top_gainers_snapshot(self):
        """æ„å»ºæ¶¨è·Œå¹…æ¦œå¿«ç…§ï¼ˆä¸å¤„ç†é”ä¸å†·å´ï¼‰ã€‚"""
        stage_started_at = time.perf_counter()
        now_utc = datetime.now(timezone.utc)
        midnight_utc = now_utc.replace(hour=0, minute=0, second=0, microsecond=0)
        midnight_utc_ms = int(midnight_utc.timestamp() * 1000)

        exchange_info = self.processor.get_exchange_info(client=self.processor.client)
        if not exchange_info or 'symbols' not in exchange_info:
            raise RuntimeError("æ— æ³•è·å– exchangeInfo")

        usdt_perpetual_symbols = {
            item.get('symbol')
            for item in exchange_info.get('symbols', [])
            if (
                item.get('contractType') == 'PERPETUAL'
                and item.get('quoteAsset') == 'USDT'
                and str(item.get('status', '')).upper() == 'TRADING'
            )
        }
        if not usdt_perpetual_symbols:
            raise RuntimeError("æ— å¯ç”¨USDTæ°¸ç»­äº¤æ˜“å¯¹")

        ticker_data = self.processor.client.public_get('/fapi/v1/ticker/24hr')
        if not ticker_data or not isinstance(ticker_data, list):
            raise RuntimeError("æ— æ³•è·å– 24hr ticker")

        candidates = []
        for item in ticker_data:
            symbol = item.get('symbol')
            if not symbol or symbol not in usdt_perpetual_symbols:
                continue
            try:
                last_price = float(item.get('lastPrice', 0.0))
                quote_volume = float(item.get('quoteVolume', 0.0))
            except (TypeError, ValueError):
                continue

            if last_price <= 0:
                continue
            if quote_volume < self.leaderboard_min_quote_volume:
                continue

            candidates.append({
                'symbol': symbol,
                'last_price': last_price,
                'quote_volume': quote_volume,
            })

        # å…ˆæŒ‰æˆäº¤é¢æ’åºï¼›å¦‚é…ç½®äº†ä¸Šé™åˆ™æˆªæ–­ï¼Œé¿å…Kçº¿è¯·æ±‚è¿‡å¤š
        candidates.sort(key=lambda x: x['quote_volume'], reverse=True)
        if self.leaderboard_max_symbols > 0:
            candidates = candidates[:self.leaderboard_max_symbols]
        logger.info(
            "æ™¨é—´æ¶¨å¹…æ¦œå€™é€‰ç»Ÿè®¡: "
            f"candidates={len(candidates)}, "
            f"min_quote_volume={self.leaderboard_min_quote_volume:.0f}, "
            f"max_symbols={self.leaderboard_max_symbols}"
        )

        leaderboard = []
        progress_step = 20
        total_candidates = len(candidates)
        if total_candidates > 0:
            # Binance Futures REST REQUEST_WEIGHT limit: 2400/min.
            # Klines(limit=1) weight=1; we reserve part of budget for other jobs and keep a conservative cap.
            min_interval = max(0.05, float(os.getenv("BINANCE_MIN_REQUEST_INTERVAL", "0.3")))
            per_worker_rpm = max(1.0, 60.0 / min_interval)  # each request here costs weight=1
            workers_by_budget = max(1, int(self.leaderboard_weight_budget_per_minute // per_worker_rpm))
            worker_count = min(total_candidates, self.leaderboard_kline_workers, workers_by_budget)
            estimated_peak_weight_per_min = int(worker_count * per_worker_rpm)
            estimated_total_weight = 1 + 40 + total_candidates  # exchangeInfo + 24hr ticker + per-symbol klines
            logger.info(
                "æ™¨é—´æ¶¨å¹…æ¦œå¹¶å‘è®¡åˆ’: "
                f"workers={worker_count}, "
                f"min_interval={min_interval:.2f}s, "
                f"budget={self.leaderboard_weight_budget_per_minute}/min, "
                f"est_peak={estimated_peak_weight_per_min}/min, "
                f"est_total_weight={estimated_total_weight}"
            )

            thread_local = threading.local()

            def _kline_task(item: dict):
                if self._is_api_cooldown_active(source='æ¶¨å¹…æ¦œ-é€å¸ç§è®¡ç®—'):
                    return item, None
                worker_client = getattr(thread_local, "client", None)
                if worker_client is None:
                    worker_client = self.processor._create_worker_client()
                    thread_local.client = worker_client
                open_price = self.processor.get_price_change_from_utc_start(
                    symbol=item['symbol'],
                    timestamp=midnight_utc_ms,
                    client=worker_client
                )
                return item, open_price

            processed = 0
            with ThreadPoolExecutor(max_workers=worker_count) as executor:
                futures = [executor.submit(_kline_task, item) for item in candidates]
                for future in as_completed(futures):
                    processed += 1
                    try:
                        item, open_price = future.result()
                    except Exception as exc:
                        logger.warning(f"æ¶¨å¹…æ¦œé€å¸ç§è®¡ç®—å¼‚å¸¸: {exc}")
                        if processed % progress_step == 0 or processed == total_candidates:
                            logger.info(
                                "æ™¨é—´æ¶¨å¹…æ¦œè¿›åº¦: "
                                f"{processed}/{total_candidates}, "
                                f"effective={len(leaderboard)}, "
                                f"elapsed={time.perf_counter() - stage_started_at:.1f}s"
                            )
                        continue

                    if open_price is not None and open_price > 0:
                        pct_change = (item['last_price'] / open_price - 1) * 100
                        leaderboard.append({
                            'symbol': item['symbol'],
                            'change': pct_change,
                            'volume': item['quote_volume'],
                            'last_price': item['last_price'],
                        })

                    if processed % progress_step == 0 or processed == total_candidates:
                        logger.info(
                            "æ™¨é—´æ¶¨å¹…æ¦œè¿›åº¦: "
                            f"{processed}/{total_candidates}, "
                            f"effective={len(leaderboard)}, "
                            f"elapsed={time.perf_counter() - stage_started_at:.1f}s"
                        )

        leaderboard.sort(key=lambda x: x['change'], reverse=True)
        top_list = leaderboard[:self.leaderboard_top_n]
        losers_list = sorted(leaderboard, key=lambda x: x['change'])[:self.leaderboard_top_n]

        snapshot = {
            "snapshot_date": datetime.now(UTC8).strftime('%Y-%m-%d'),
            "snapshot_time": datetime.now(UTC8).strftime('%Y-%m-%d %H:%M:%S'),
            "window_start_utc": midnight_utc.strftime('%Y-%m-%d %H:%M:%S'),
            "candidates": len(candidates),
            "effective": len(leaderboard),
            "top": len(top_list),
            "rows": top_list,
            "losers_rows": losers_list,
            "all_rows": leaderboard,
        }
        logger.info(
            "æ™¨é—´æ¶¨å¹…æ¦œå¿«ç…§æ„å»ºå®Œæˆ: "
            f"candidates={snapshot['candidates']}, "
            f"effective={snapshot['effective']}, "
            f"top={snapshot['top']}, "
            f"elapsed={time.perf_counter() - stage_started_at:.1f}s"
        )
        return snapshot

    def get_top_gainers_snapshot(self, source: str = "æ¶¨å¹…æ¦œæ¥å£"):
        """è·å–æ¶¨å¹…æ¦œå¿«ç…§ï¼ˆå¸¦å†·å´ä¸äº’æ–¥ä¿æŠ¤ï¼‰ï¼Œä¾›APIæˆ–ä»»åŠ¡å¤ç”¨ã€‚"""
        if not self.processor:
            return {"ok": False, "reason": "api_keys_missing", "message": "APIå¯†é’¥æœªé…ç½®"}
        if self._is_api_cooldown_active(source=source):
            return {"ok": False, "reason": "cooldown_active", "message": "Binance APIå¤„äºå†·å´ä¸­"}
        if not self._try_enter_api_job_slot(source=source):
            return {"ok": False, "reason": "lock_busy", "message": "ä»»åŠ¡æ§½ä½ç¹å¿™"}

        try:
            snapshot = self._build_top_gainers_snapshot()
            if snapshot["top"] <= 0:
                return {"ok": False, "reason": "no_data", "message": "æœªç”Ÿæˆæœ‰æ•ˆæ¦œå•", **snapshot}
            return {"ok": True, **snapshot}
        except Exception as e:
            logger.error(f"{source}å¤±è´¥: {e}")
            return {"ok": False, "reason": "exception", "message": str(e)}
        finally:
            self._release_api_job_slot()

    def send_morning_top_gainers(self):
        """æ¯å¤©æ—©ä¸Šå‘é€å¸å®‰åˆçº¦æ¶¨è·Œå¹…æ¦œï¼ˆæŒ‰UTCå½“æ—¥å¼€ç›˜åˆ°å½“å‰æ¶¨è·Œå¹…ï¼‰"""
        started_at = time.perf_counter()
        logger.info(
            "æ™¨é—´æ¶¨å¹…æ¦œä»»åŠ¡å¼€å§‹æ‰§è¡Œ: "
            f"schedule={self.leaderboard_alert_hour:02d}:{self.leaderboard_alert_minute:02d}"
        )
        result = self.get_top_gainers_snapshot(source="æ™¨é—´æ¶¨å¹…æ¦œ")
        logger.info(
            "æ™¨é—´æ¶¨å¹…æ¦œå¿«ç…§ç»“æœ: "
            f"ok={result.get('ok')}, "
            f"reason={result.get('reason', '')}, "
            f"candidates={result.get('candidates', 0)}, "
            f"effective={result.get('effective', 0)}, "
            f"top={result.get('top', 0)}"
        )
        if not result.get("ok"):
            logger.warning(
                f"æ™¨é—´æ¶¨å¹…æ¦œä»»åŠ¡è·³è¿‡: reason={result.get('reason')}, message={result.get('message', '')}"
            )
            return

        try:
            self.db.save_leaderboard_snapshot(result)
            logger.info(
                f"æ¶¨å¹…æ¦œå¿«ç…§å·²ä¿å­˜: date={result.get('snapshot_date')}, top={result.get('top')}"
            )
        except Exception as e:
            logger.error(f"ä¿å­˜æ¶¨å¹…æ¦œå¿«ç…§å¤±è´¥: {e}")

        try:
            metrics_payload = self.db.upsert_leaderboard_daily_metrics_for_date(
                str(result.get("snapshot_date"))
            )
            if metrics_payload:
                logger.info(
                    "æ¶¨è·Œå¹…æŒ‡æ ‡å·²ä¿å­˜: "
                    f"date={result.get('snapshot_date')}, "
                    f"m1={metrics_payload.get('metric1', {}).get('probability_pct')}, "
                    f"m2={metrics_payload.get('metric2', {}).get('probability_pct')}, "
                    f"m3_eval={metrics_payload.get('metric3', {}).get('evaluated_count')}"
                )
        except Exception as e:
            logger.error(f"ä¿å­˜æ¶¨è·Œå¹…æŒ‡æ ‡å¤±è´¥: {e}")

        title = f"ã€å¸å®‰åˆçº¦å¸‚åœºæ¶¨è·Œå¹…æ¦œ Top {result['top']}ã€‘"
        content = (
            "### å¸å®‰åˆçº¦å¸‚åœºæ™¨é—´æ¶¨è·Œå¹…æ¦œ\n\n"
            f"**æ›´æ–°æ—¶é—´:** {result['snapshot_time']} (UTC+8)\n"
            f"**è®¡ç®—åŒºé—´:** {result['window_start_utc']} UTC è‡³å½“å‰\n\n"
            "#### æ¶¨å¹…æ¦œ Top10\n\n"
            "| æ’å | å¸ç§ | æ¶¨å¹… | 24hæˆäº¤é¢ |\n"
            "|:---:|:---:|:---:|:---:|\n"
        )

        for i, row in enumerate(result["rows"], start=1):
            symbol = row['symbol']
            change = f"{row['change']:.2f}%"
            volume = f"{int(row['volume'] / 1_000_000)}M"
            content += f"| {i} | {symbol} | {change} | {volume} |\n"

        losers_rows = result.get("losers_rows", [])
        if losers_rows:
            content += (
                "\n#### è·Œå¹…æ¦œ Top10\n\n"
                "| æ’å | å¸ç§ | è·Œå¹… | 24hæˆäº¤é¢ |\n"
                "|:---:|:---:|:---:|:---:|\n"
            )
            for i, row in enumerate(losers_rows, start=1):
                symbol = row['symbol']
                change = f"{row['change']:.2f}%"
                volume = f"{int(row['volume'] / 1_000_000)}M"
                content += f"| {i} | {symbol} | {change} | {volume} |\n"

        send_server_chan_notification(title, content)
        logger.info(
            "æ™¨é—´æ¶¨å¹…æ¦œå·²å‘é€: "
            f"candidates={result['candidates']}, "
            f"effective={result['effective']}, "
            f"top={result['top']}, "
            f"losers_top={len(result.get('losers_rows', []))}, "
            f"elapsed={time.perf_counter() - started_at:.2f}s"
        )

    def _build_rebound_snapshot(
        self,
        *,
        window_days: int,
        top_n: int,
        kline_workers: int,
        weight_budget_per_minute: int,
        label: str
    ):
        """æ„å»ºåå¼¹å¹…åº¦æ¦œå¿«ç…§ï¼ˆä¸å¤„ç†é”ä¸å†·å´ï¼‰ã€‚"""
        stage_started_at = time.perf_counter()
        now_utc = datetime.now(timezone.utc)
        window_start_utc = now_utc - timedelta(days=window_days)

        exchange_info = self.processor.get_exchange_info(client=self.processor.client)
        if not exchange_info or 'symbols' not in exchange_info:
            raise RuntimeError("æ— æ³•è·å– exchangeInfo")

        usdt_perpetual_symbols = {
            item.get('symbol')
            for item in exchange_info.get('symbols', [])
            if (
                item.get('contractType') == 'PERPETUAL'
                and item.get('quoteAsset') == 'USDT'
                and str(item.get('status', '')).upper() == 'TRADING'
            )
        }
        if not usdt_perpetual_symbols:
            raise RuntimeError("æ— å¯ç”¨USDTæ°¸ç»­äº¤æ˜“å¯¹")

        ticker_data = self.processor.client.public_get('/fapi/v1/ticker/price')
        if not ticker_data:
            raise RuntimeError("æ— æ³•è·å– ticker/price")
        if isinstance(ticker_data, dict):
            ticker_data = [ticker_data]

        candidates = []
        for item in ticker_data:
            symbol = item.get('symbol')
            if not symbol or symbol not in usdt_perpetual_symbols:
                continue
            try:
                current_price = float(item.get('price', 0.0))
            except (TypeError, ValueError):
                continue
            if current_price <= 0:
                continue
            candidates.append({
                'symbol': symbol,
                'current_price': current_price,
            })

        # ç¨³å®šæ’åºï¼Œç¡®ä¿åŒç­‰æ¡ä»¶ä¸‹è¾“å‡ºä¸€è‡´
        candidates.sort(key=lambda x: x['symbol'])
        logger.info(
            f"{label}å€™é€‰ç»Ÿè®¡: "
            f"candidates={len(candidates)}, "
            f"top_n={top_n}"
        )

        rebound_rows = []
        progress_step = 20
        total_candidates = len(candidates)
        if total_candidates > 0:
            min_interval = max(0.05, float(os.getenv("BINANCE_MIN_REQUEST_INTERVAL", "0.3")))
            per_worker_rpm = max(1.0, 60.0 / min_interval)
            workers_by_budget = max(1, int(weight_budget_per_minute // per_worker_rpm))
            worker_count = min(total_candidates, kline_workers, workers_by_budget)
            estimated_peak_weight_per_min = int(worker_count * per_worker_rpm)
            estimated_total_weight = 1 + 1 + total_candidates  # exchangeInfo + ticker/price + per-symbol klines
            logger.info(
                f"{label}å¹¶å‘è®¡åˆ’: "
                f"workers={worker_count}, "
                f"min_interval={min_interval:.2f}s, "
                f"budget={weight_budget_per_minute}/min, "
                f"est_peak={estimated_peak_weight_per_min}/min, "
                f"est_total_weight={estimated_total_weight}"
            )

            thread_local = threading.local()
            metric_field = f"rebound_{window_days}d_pct"
            low_field = f"low_{window_days}d"
            low_time_field = f"low_{window_days}d_at_utc"
            kline_limit = max(14, int(window_days))

            def _kline_task(item: dict):
                if self._is_api_cooldown_active(source=f'{label}-é€å¸ç§è®¡ç®—'):
                    return item, None

                worker_client = getattr(thread_local, "client", None)
                if worker_client is None:
                    worker_client = self.processor._create_worker_client()
                    thread_local.client = worker_client

                try:
                    klines = worker_client.public_get('/fapi/v1/klines', {
                        'symbol': item['symbol'],
                        'interval': '1d',
                        'limit': kline_limit
                    }) or []
                except Exception:
                    return item, None

                lows = []
                for kline in klines:
                    if not isinstance(kline, list) or len(kline) < 4:
                        continue
                    try:
                        low_price = float(kline[3])
                        open_time = int(kline[0])
                    except (TypeError, ValueError):
                        continue
                    if low_price <= 0:
                        continue
                    lows.append((low_price, open_time))

                if not lows:
                    return item, None

                low_price, low_ts = min(lows, key=lambda entry: entry[0])
                rebound_pct = (item['current_price'] / low_price - 1.0) * 100.0
                low_at_utc = datetime.fromtimestamp(
                    low_ts / 1000, tz=timezone.utc
                ).strftime('%Y-%m-%d %H:%M:%S')
                return item, {
                    low_field: low_price,
                    low_time_field: low_at_utc,
                    metric_field: rebound_pct,
                }

            processed = 0
            with ThreadPoolExecutor(max_workers=worker_count) as executor:
                futures = [executor.submit(_kline_task, item) for item in candidates]
                for future in as_completed(futures):
                    processed += 1
                    try:
                        item, payload = future.result()
                    except Exception as exc:
                        logger.warning(f"{label}é€å¸ç§è®¡ç®—å¼‚å¸¸: {exc}")
                        payload = None
                        item = None

                    if item and payload:
                        rebound_rows.append({
                            'symbol': item['symbol'],
                            'current_price': item['current_price'],
                            low_field: payload[low_field],
                            low_time_field: payload[low_time_field],
                            metric_field: payload[metric_field],
                        })

                    if processed % progress_step == 0 or processed == total_candidates:
                        logger.info(
                            f"{label}è¿›åº¦: "
                            f"{processed}/{total_candidates}, "
                            f"effective={len(rebound_rows)}, "
                            f"elapsed={time.perf_counter() - stage_started_at:.1f}s"
                        )

        metric_field = f"rebound_{window_days}d_pct"
        rebound_rows.sort(key=lambda x: x[metric_field], reverse=True)
        top_list = rebound_rows[:top_n]

        snapshot = {
            "snapshot_date": datetime.now(UTC8).strftime('%Y-%m-%d'),
            "snapshot_time": datetime.now(UTC8).strftime('%Y-%m-%d %H:%M:%S'),
            "window_start_utc": window_start_utc.strftime('%Y-%m-%d %H:%M:%S'),
            "candidates": len(candidates),
            "effective": len(rebound_rows),
            "top": len(top_list),
            "rows": top_list,
            "all_rows": rebound_rows,
        }
        logger.info(
            f"{label}å¿«ç…§æ„å»ºå®Œæˆ: "
            f"candidates={snapshot['candidates']}, "
            f"effective={snapshot['effective']}, "
            f"top={snapshot['top']}, "
            f"elapsed={time.perf_counter() - stage_started_at:.1f}s"
        )
        return snapshot

    def _build_rebound_7d_snapshot(self):
        """æ„å»º14Dåå¼¹å¹…åº¦æ¦œå¿«ç…§ï¼ˆå…¼å®¹å†å²å‡½æ•°åï¼‰ã€‚"""
        return self._build_rebound_snapshot(
            window_days=14,
            top_n=self.rebound_7d_top_n,
            kline_workers=self.rebound_7d_kline_workers,
            weight_budget_per_minute=self.rebound_7d_weight_budget_per_minute,
            label="14Dåå¼¹æ¦œ"
        )

    def _build_rebound_30d_snapshot(self):
        """æ„å»º30Dåå¼¹å¹…åº¦æ¦œå¿«ç…§ã€‚"""
        return self._build_rebound_snapshot(
            window_days=30,
            top_n=self.rebound_30d_top_n,
            kline_workers=self.rebound_30d_kline_workers,
            weight_budget_per_minute=self.rebound_30d_weight_budget_per_minute,
            label="30Dåå¼¹æ¦œ"
        )

    def _build_rebound_60d_snapshot(self):
        """æ„å»º60Dåå¼¹å¹…åº¦æ¦œå¿«ç…§ã€‚"""
        return self._build_rebound_snapshot(
            window_days=60,
            top_n=self.rebound_60d_top_n,
            kline_workers=self.rebound_60d_kline_workers,
            weight_budget_per_minute=self.rebound_60d_weight_budget_per_minute,
            label="60Dåå¼¹æ¦œ"
        )

    def get_rebound_7d_snapshot(self, source: str = "14Dåå¼¹æ¦œæ¥å£"):
        """è·å–14Dåå¼¹æ¦œå¿«ç…§ï¼ˆå¸¦å†·å´ä¸äº’æ–¥ä¿æŠ¤ï¼‰ï¼Œä¾›APIæˆ–ä»»åŠ¡å¤ç”¨ã€‚"""
        if not self.processor:
            return {"ok": False, "reason": "api_keys_missing", "message": "APIå¯†é’¥æœªé…ç½®"}
        if self._is_api_cooldown_active(source=source):
            return {"ok": False, "reason": "cooldown_active", "message": "Binance APIå¤„äºå†·å´ä¸­"}
        if not self._try_enter_api_job_slot(source=source):
            return {"ok": False, "reason": "lock_busy", "message": "ä»»åŠ¡æ§½ä½ç¹å¿™"}

        try:
            snapshot = self._build_rebound_7d_snapshot()
            if snapshot["top"] <= 0:
                return {"ok": False, "reason": "no_data", "message": "æœªç”Ÿæˆæœ‰æ•ˆæ¦œå•", **snapshot}
            return {"ok": True, **snapshot}
        except Exception as e:
            logger.error(f"{source}å¤±è´¥: {e}")
            return {"ok": False, "reason": "exception", "message": str(e)}
        finally:
            self._release_api_job_slot()

    def snapshot_morning_rebound_7d(self):
        """æ¯å¤©æ—©ä¸Š07:30ç”Ÿæˆ14Dåå¼¹å¹…åº¦Topæ¦œå¿«ç…§å¹¶å…¥åº“ã€‚"""
        started_at = time.perf_counter()
        logger.info(
            "æ™¨é—´14Dåå¼¹æ¦œä»»åŠ¡å¼€å§‹æ‰§è¡Œ: "
            f"schedule={self.rebound_7d_hour:02d}:{self.rebound_7d_minute:02d}"
        )
        result = self.get_rebound_7d_snapshot(source="æ™¨é—´14Dåå¼¹æ¦œ")
        logger.info(
            "æ™¨é—´14Dåå¼¹æ¦œå¿«ç…§ç»“æœ: "
            f"ok={result.get('ok')}, "
            f"reason={result.get('reason', '')}, "
            f"candidates={result.get('candidates', 0)}, "
            f"effective={result.get('effective', 0)}, "
            f"top={result.get('top', 0)}"
        )
        if not result.get("ok"):
            logger.warning(
                f"æ™¨é—´14Dåå¼¹æ¦œä»»åŠ¡è·³è¿‡: reason={result.get('reason')}, message={result.get('message', '')}"
            )
            return

        try:
            self.db.save_rebound_7d_snapshot(result)
            logger.info(
                f"14Dåå¼¹æ¦œå¿«ç…§å·²ä¿å­˜: date={result.get('snapshot_date')}, top={result.get('top')}"
            )
        except Exception as e:
            logger.error(f"ä¿å­˜14Dåå¼¹æ¦œå¿«ç…§å¤±è´¥: {e}")

        logger.info(
            "æ™¨é—´14Dåå¼¹æ¦œä»»åŠ¡å®Œæˆ: "
            f"elapsed={time.perf_counter() - started_at:.2f}s"
        )

    def get_rebound_30d_snapshot(self, source: str = "30Dåå¼¹æ¦œæ¥å£"):
        """è·å–30Dåå¼¹æ¦œå¿«ç…§ï¼ˆå¸¦å†·å´ä¸äº’æ–¥ä¿æŠ¤ï¼‰ï¼Œä¾›APIæˆ–ä»»åŠ¡å¤ç”¨ã€‚"""
        if not self.processor:
            return {"ok": False, "reason": "api_keys_missing", "message": "APIå¯†é’¥æœªé…ç½®"}
        if self._is_api_cooldown_active(source=source):
            return {"ok": False, "reason": "cooldown_active", "message": "Binance APIå¤„äºå†·å´ä¸­"}
        if not self._try_enter_api_job_slot(source=source):
            return {"ok": False, "reason": "lock_busy", "message": "ä»»åŠ¡æ§½ä½ç¹å¿™"}

        try:
            snapshot = self._build_rebound_30d_snapshot()
            if snapshot["top"] <= 0:
                return {"ok": False, "reason": "no_data", "message": "æœªç”Ÿæˆæœ‰æ•ˆæ¦œå•", **snapshot}
            return {"ok": True, **snapshot}
        except Exception as e:
            logger.error(f"{source}å¤±è´¥: {e}")
            return {"ok": False, "reason": "exception", "message": str(e)}
        finally:
            self._release_api_job_slot()

    def snapshot_morning_rebound_30d(self):
        """æ¯å¤©æ—©ä¸Šç”Ÿæˆ30Dåå¼¹å¹…åº¦Topæ¦œå¿«ç…§å¹¶å…¥åº“ã€‚"""
        started_at = time.perf_counter()
        logger.info(
            "æ™¨é—´30Dåå¼¹æ¦œä»»åŠ¡å¼€å§‹æ‰§è¡Œ: "
            f"schedule={self.rebound_30d_hour:02d}:{self.rebound_30d_minute:02d}"
        )
        result = self.get_rebound_30d_snapshot(source="æ™¨é—´30Dåå¼¹æ¦œ")
        logger.info(
            "æ™¨é—´30Dåå¼¹æ¦œå¿«ç…§ç»“æœ: "
            f"ok={result.get('ok')}, "
            f"reason={result.get('reason', '')}, "
            f"candidates={result.get('candidates', 0)}, "
            f"effective={result.get('effective', 0)}, "
            f"top={result.get('top', 0)}"
        )
        if not result.get("ok"):
            logger.warning(
                f"æ™¨é—´30Dåå¼¹æ¦œä»»åŠ¡è·³è¿‡: reason={result.get('reason')}, message={result.get('message', '')}"
            )
            return

        try:
            self.db.save_rebound_30d_snapshot(result)
            logger.info(
                f"30Dåå¼¹æ¦œå¿«ç…§å·²ä¿å­˜: date={result.get('snapshot_date')}, top={result.get('top')}"
            )
        except Exception as e:
            logger.error(f"ä¿å­˜30Dåå¼¹æ¦œå¿«ç…§å¤±è´¥: {e}")

        logger.info(
            "æ™¨é—´30Dåå¼¹æ¦œä»»åŠ¡å®Œæˆ: "
            f"elapsed={time.perf_counter() - started_at:.2f}s"
        )

    def get_rebound_60d_snapshot(self, source: str = "60Dåå¼¹æ¦œæ¥å£"):
        """è·å–60Dåå¼¹æ¦œå¿«ç…§ï¼ˆå¸¦å†·å´ä¸äº’æ–¥ä¿æŠ¤ï¼‰ï¼Œä¾›APIæˆ–ä»»åŠ¡å¤ç”¨ã€‚"""
        if not self.processor:
            return {"ok": False, "reason": "api_keys_missing", "message": "APIå¯†é’¥æœªé…ç½®"}
        if self._is_api_cooldown_active(source=source):
            return {"ok": False, "reason": "cooldown_active", "message": "Binance APIå¤„äºå†·å´ä¸­"}
        if not self._try_enter_api_job_slot(source=source):
            return {"ok": False, "reason": "lock_busy", "message": "ä»»åŠ¡æ§½ä½ç¹å¿™"}

        try:
            snapshot = self._build_rebound_60d_snapshot()
            if snapshot["top"] <= 0:
                return {"ok": False, "reason": "no_data", "message": "æœªç”Ÿæˆæœ‰æ•ˆæ¦œå•", **snapshot}
            return {"ok": True, **snapshot}
        except Exception as e:
            logger.error(f"{source}å¤±è´¥: {e}")
            return {"ok": False, "reason": "exception", "message": str(e)}
        finally:
            self._release_api_job_slot()

    def snapshot_morning_rebound_60d(self):
        """æ¯å¤©æ—©ä¸Šç”Ÿæˆ60Dåå¼¹å¹…åº¦Topæ¦œå¿«ç…§å¹¶å…¥åº“ã€‚"""
        started_at = time.perf_counter()
        logger.info(
            "æ™¨é—´60Dåå¼¹æ¦œä»»åŠ¡å¼€å§‹æ‰§è¡Œ: "
            f"schedule={self.rebound_60d_hour:02d}:{self.rebound_60d_minute:02d}"
        )
        result = self.get_rebound_60d_snapshot(source="æ™¨é—´60Dåå¼¹æ¦œ")
        logger.info(
            "æ™¨é—´60Dåå¼¹æ¦œå¿«ç…§ç»“æœ: "
            f"ok={result.get('ok')}, "
            f"reason={result.get('reason', '')}, "
            f"candidates={result.get('candidates', 0)}, "
            f"effective={result.get('effective', 0)}, "
            f"top={result.get('top', 0)}"
        )
        if not result.get("ok"):
            logger.warning(
                f"æ™¨é—´60Dåå¼¹æ¦œä»»åŠ¡è·³è¿‡: reason={result.get('reason')}, message={result.get('message', '')}"
            )
            return

        try:
            self.db.save_rebound_60d_snapshot(result)
            logger.info(
                f"60Dåå¼¹æ¦œå¿«ç…§å·²ä¿å­˜: date={result.get('snapshot_date')}, top={result.get('top')}"
            )
        except Exception as e:
            logger.error(f"ä¿å­˜60Dåå¼¹æ¦œå¿«ç…§å¤±è´¥: {e}")

        logger.info(
            "æ™¨é—´60Dåå¼¹æ¦œä»»åŠ¡å®Œæˆ: "
            f"elapsed={time.perf_counter() - started_at:.2f}s"
        )

    @staticmethod
    def _normalize_futures_symbol(symbol: str) -> str:
        """å°†åº“å†…symbolè§„èŒƒåŒ–ä¸ºBinance USDTäº¤æ˜“å¯¹symbol"""
        sym = str(symbol or "").upper().strip()
        if not sym:
            return sym
        if sym.endswith("USDT") or sym.endswith("BUSD"):
            return sym
        return f"{sym}USDT"

    def sync_balance_data(self):
        """åŒæ­¥è´¦æˆ·ä½™é¢æ•°æ®åˆ°æ•°æ®åº“"""
        if not self.processor:
            return  # å¦‚æœæ²¡æœ‰é…ç½®APIå¯†é’¥ï¼Œåˆ™ä¸æ‰§è¡Œ
        if self._is_api_cooldown_active(source='ä½™é¢åŒæ­¥'):
            return
        if not self._try_enter_api_job_slot(source='ä½™é¢åŒæ­¥'):
            return

        try:
            logger.info("å¼€å§‹åŒæ­¥è´¦æˆ·ä½™é¢...")
            # balance_info returns {'margin_balance': float, 'wallet_balance': float}
            balance_info = self.processor.get_account_balance()

            if balance_info:
                current_margin = balance_info['margin_balance']
                current_wallet = balance_info['wallet_balance']

                # --- é€šè¿‡ Binance income API ç›´æ¥åŒæ­¥å‡ºå…¥é‡‘ ---
                try:
                    latest_event_time_ms = self.db.get_latest_transfer_event_time()
                    if latest_event_time_ms is None:
                        lookback_days_raw = os.getenv('TRANSFER_SYNC_LOOKBACK_DAYS', '90')
                        try:
                            lookback_days = max(1, int(lookback_days_raw))
                        except ValueError:
                            logger.warning(
                                f"TRANSFER_SYNC_LOOKBACK_DAYS={lookback_days_raw} éæ³•ï¼Œå›é€€ä¸º 90"
                            )
                            lookback_days = 90
                        start_time_ms = int(
                            (datetime.now(timezone.utc) - timedelta(days=lookback_days)).timestamp() * 1000
                        )
                    else:
                        # å¾€å‰å›çœ‹1åˆ†é’Ÿåšè¾¹ç•Œä¿æŠ¤ï¼Œè½åº“ä¾§ä¼šæŒ‰ source_uid å»é‡
                        start_time_ms = max(0, latest_event_time_ms - 60_000)

                    end_time_ms = int(datetime.now(timezone.utc).timestamp() * 1000)
                    transfer_rows = self.processor.get_transfer_income_records(
                        start_time=start_time_ms,
                        end_time=end_time_ms
                    )

                    inserted_count = 0
                    for row in transfer_rows:
                        inserted = self.db.save_transfer_income(
                            amount=row['amount'],
                            event_time=row['event_time_ms'],
                            asset=row.get('asset') or 'USDT',
                            income_type=row.get('income_type') or 'TRANSFER',
                            source_uid=row.get('source_uid'),
                            description=row.get('description')
                        )
                        if inserted:
                            inserted_count += 1

                    logger.info(
                        "å‡ºå…¥é‡‘åŒæ­¥å®Œæˆ: "
                        f"fetched={len(transfer_rows)}, inserted={inserted_count}, "
                        f"window={self._format_window_with_ms(start_time_ms, end_time_ms)}"
                    )
                except Exception as e:
                    logger.warning(f"å‡ºå…¥é‡‘åŒæ­¥å‡ºé”™: {e}")

                # ä¿å­˜å½“å‰çŠ¶æ€
                self.db.save_balance_history(current_margin, current_wallet)
                logger.info(f"ä½™é¢å·²æ›´æ–°: {current_margin:.2f} USDT (Wallet: {current_wallet:.2f})")
            else:
                logger.warning("è·å–ä½™é¢å¤±è´¥ï¼Œbalanceä¸º None")
        except Exception as e:
            logger.error(f"åŒæ­¥ä½™é¢å¤±è´¥: {str(e)}")
        finally:
            self._release_api_job_slot()

    def start(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
        if not self.processor:
            logger.warning("å®šæ—¶ä»»åŠ¡æœªå¯åŠ¨: APIå¯†é’¥æœªé…ç½®")
            return

        # ç«‹å³æ‰§è¡Œä¸€æ¬¡åŒæ­¥
        logger.info("ç«‹å³æ‰§è¡Œé¦–æ¬¡æ•°æ®åŒæ­¥...")
        self.scheduler.add_job(partial(run_sync_trades_incremental, self), 'date')
        self.scheduler.add_job(partial(run_sync_open_positions, self), 'date')
        self.scheduler.add_job(self.sync_balance_data, 'date')

        # å¢é‡åŒæ­¥ä»»åŠ¡ - æ¯éš”Nåˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
        self.scheduler.add_job(
            func=partial(run_sync_trades_incremental, self),
            trigger=IntervalTrigger(minutes=self.update_interval_minutes),
            id='sync_trades_incremental',
            name='åŒæ­¥äº¤æ˜“æ•°æ®(å¢é‡)',
            max_instances=1,
            coalesce=True,
            misfire_grace_time=120,
            replace_existing=True
        )

        # æœªå¹³ä»“åŒæ­¥ä»»åŠ¡ - ä¸é—­ä»“ETLè§£è€¦
        self.scheduler.add_job(
            func=partial(run_sync_open_positions, self),
            trigger=IntervalTrigger(minutes=self.open_positions_update_interval_minutes),
            id='sync_open_positions',
            name='åŒæ­¥æœªå¹³ä»“è®¢å•',
            max_instances=1,
            coalesce=True,
            misfire_grace_time=120,
            replace_existing=True
        )

        # æ¯æ—¥å…¨é‡åŒæ­¥ä»»åŠ¡ - é»˜è®¤æ¯å¤© 03:30 (UTC+8)
        if self.enable_daily_full_sync:
            self.scheduler.add_job(
                func=self.sync_trades_full,
                trigger=CronTrigger(
                    hour=self.daily_full_sync_hour,
                    minute=self.daily_full_sync_minute,
                    timezone=UTC8
                ),
                id='sync_trades_full_daily',
                name='åŒæ­¥äº¤æ˜“æ•°æ®(å…¨é‡)',
                max_instances=1,
                coalesce=True,
                misfire_grace_time=600,
                replace_existing=True
            )
            logger.info(
                "å…¨é‡åŒæ­¥ä»»åŠ¡å·²å¯åŠ¨: "
                f"æ¯å¤© {self.daily_full_sync_hour:02d}:{self.daily_full_sync_minute:02d} æ‰§è¡Œ"
            )
        else:
            logger.info("å…¨é‡åŒæ­¥ä»»åŠ¡æœªå¯ç”¨: ENABLE_DAILY_FULL_SYNC=0")

        if not self.enable_user_stream:
            # æ·»åŠ ä½™é¢åŒæ­¥ä»»åŠ¡ - æ¯åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
            self.scheduler.add_job(
                func=self.sync_balance_data,
                trigger=IntervalTrigger(minutes=1),
                id='sync_balance',
                name='åŒæ­¥è´¦æˆ·ä½™é¢',
                max_instances=1,
                coalesce=True,
                misfire_grace_time=60,
                replace_existing=True
            )
        else:
            logger.info("å·²å¯ç”¨ç”¨æˆ·æ•°æ®æµï¼Œè·³è¿‡è½®è¯¢ä½™é¢åŒæ­¥ä»»åŠ¡")

        # æ·»åŠ ç¡å‰é£æ§æ£€æŸ¥ä»»åŠ¡ - æ¯å¤© 23:00 (UTC+8) æ‰§è¡Œ
        self.scheduler.add_job(
            func=self.check_risk_before_sleep,
            trigger=CronTrigger(hour=23, minute=0, timezone=UTC8),
            id='risk_check_sleep',
            name='ç¡å‰é£æ§æ£€æŸ¥',
            max_instances=1,
            coalesce=True,
            misfire_grace_time=300,
            replace_existing=True
        )

        # æ·»åŠ åˆé—´æ­¢æŸå¤ç›˜ä»»åŠ¡ - æ¯å¤© 23:02 (UTC+8) æ‰§è¡Œï¼ˆé»˜è®¤ï¼‰
        self.scheduler.add_job(
            func=self.review_noon_loss_at_night,
            trigger=CronTrigger(
                hour=self.noon_review_hour,
                minute=self.noon_review_minute,
                timezone=UTC8
            ),
            id='review_noon_loss_night',
            name='åˆé—´æ­¢æŸå¤œé—´å¤ç›˜',
            max_instances=1,
            coalesce=True,
            misfire_grace_time=300,
            replace_existing=True
        )

        # æ·»åŠ åˆé—´æµ®äºæ£€æŸ¥ä»»åŠ¡ - é»˜è®¤æ¯å¤© 11:50 (UTC+8) æ‰§è¡Œ
        self.scheduler.add_job(
            func=partial(run_noon_loss_check, self),
            trigger=CronTrigger(
                hour=self.noon_loss_check_hour,
                minute=self.noon_loss_check_minute,
                timezone=UTC8
            ),
            id='check_losses_noon',
            name='åˆé—´æµ®äºæ£€æŸ¥',
            max_instances=1,
            coalesce=True,
            misfire_grace_time=300,
            replace_existing=True
        )

        if self.enable_leaderboard_alert:
            self.scheduler.add_job(
                func=self.send_morning_top_gainers,
                trigger=CronTrigger(
                    hour=self.leaderboard_alert_hour,
                    minute=self.leaderboard_alert_minute,
                    timezone=UTC8
                ),
                id='send_morning_top_gainers',
                name='æ™¨é—´æ¶¨å¹…æ¦œ',
                max_instances=1,
                coalesce=True,
                misfire_grace_time=300,
                replace_existing=True
            )
            logger.info(
                "æ™¨é—´æ¶¨å¹…æ¦œä»»åŠ¡å·²å¯åŠ¨: "
                f"æ¯å¤© {self.leaderboard_alert_hour:02d}:{self.leaderboard_alert_minute:02d} æ‰§è¡Œ"
            )
        else:
            logger.info("æ™¨é—´æ¶¨å¹…æ¦œä»»åŠ¡æœªå¯ç”¨: ENABLE_LEADERBOARD_ALERT=0")

        if self.enable_rebound_7d_snapshot:
            self.scheduler.add_job(
                func=self.snapshot_morning_rebound_7d,
                trigger=CronTrigger(
                    hour=self.rebound_7d_hour,
                    minute=self.rebound_7d_minute,
                    timezone=UTC8
                ),
                id='snapshot_morning_rebound_7d',
                name='æ™¨é—´14Dåå¼¹æ¦œ',
                max_instances=1,
                coalesce=True,
                misfire_grace_time=300,
                replace_existing=True
            )
            logger.info(
                "æ™¨é—´14Dåå¼¹æ¦œä»»åŠ¡å·²å¯åŠ¨: "
                f"æ¯å¤© {self.rebound_7d_hour:02d}:{self.rebound_7d_minute:02d} æ‰§è¡Œ"
            )
        else:
            logger.info("æ™¨é—´14Dåå¼¹æ¦œä»»åŠ¡æœªå¯ç”¨: ENABLE_REBOUND_7D_SNAPSHOT=0")

        if self.enable_rebound_30d_snapshot:
            self.scheduler.add_job(
                func=self.snapshot_morning_rebound_30d,
                trigger=CronTrigger(
                    hour=self.rebound_30d_hour,
                    minute=self.rebound_30d_minute,
                    timezone=UTC8
                ),
                id='snapshot_morning_rebound_30d',
                name='æ™¨é—´30Dåå¼¹æ¦œ',
                max_instances=1,
                coalesce=True,
                misfire_grace_time=300,
                replace_existing=True
            )
            logger.info(
                "æ™¨é—´30Dåå¼¹æ¦œä»»åŠ¡å·²å¯åŠ¨: "
                f"æ¯å¤© {self.rebound_30d_hour:02d}:{self.rebound_30d_minute:02d} æ‰§è¡Œ"
            )
        else:
            logger.info("æ™¨é—´30Dåå¼¹æ¦œä»»åŠ¡æœªå¯ç”¨: ENABLE_REBOUND_30D_SNAPSHOT=0")

        if self.enable_rebound_60d_snapshot:
            self.scheduler.add_job(
                func=self.snapshot_morning_rebound_60d,
                trigger=CronTrigger(
                    hour=self.rebound_60d_hour,
                    minute=self.rebound_60d_minute,
                    timezone=UTC8
                ),
                id='snapshot_morning_rebound_60d',
                name='æ™¨é—´60Dåå¼¹æ¦œ',
                max_instances=1,
                coalesce=True,
                misfire_grace_time=300,
                replace_existing=True
            )
            logger.info(
                "æ™¨é—´60Dåå¼¹æ¦œä»»åŠ¡å·²å¯åŠ¨: "
                f"æ¯å¤© {self.rebound_60d_hour:02d}:{self.rebound_60d_minute:02d} æ‰§è¡Œ"
            )
        else:
            logger.info("æ™¨é—´60Dåå¼¹æ¦œä»»åŠ¡æœªå¯ç”¨: ENABLE_REBOUND_60D_SNAPSHOT=0")

        self.scheduler.start()
        logger.info(f"å¢é‡äº¤æ˜“åŒæ­¥ä»»åŠ¡å·²å¯åŠ¨: æ¯ {self.update_interval_minutes} åˆ†é’Ÿè‡ªåŠ¨æ›´æ–°ä¸€æ¬¡")
        logger.info(
            f"æœªå¹³ä»“åŒæ­¥ä»»åŠ¡å·²å¯åŠ¨: æ¯ {self.open_positions_update_interval_minutes} åˆ†é’Ÿè‡ªåŠ¨æ›´æ–°ä¸€æ¬¡ "
            f"(lookback_days={self.open_positions_lookback_days})"
        )
        logger.info("ä½™é¢ç›‘æ§ä»»åŠ¡å·²å¯åŠ¨: æ¯ 1 åˆ†é’Ÿè‡ªåŠ¨æ›´æ–°ä¸€æ¬¡")
        logger.info("ç¡å‰é£æ§æ£€æŸ¥å·²å¯åŠ¨: æ¯å¤© 23:00 æ‰§è¡Œ")
        logger.info(
            "åˆé—´æµ®äºæ£€æŸ¥å·²å¯åŠ¨: "
            f"æ¯å¤© {self.noon_loss_check_hour:02d}:{self.noon_loss_check_minute:02d} æ‰§è¡Œ"
        )
        logger.info(
            "åˆé—´æ­¢æŸå¤œé—´å¤ç›˜å·²å¯åŠ¨: "
            f"æ¯å¤© {self.noon_review_hour:02d}:{self.noon_review_minute:02d} æ‰§è¡Œ, "
            f"target_day_offset={self.noon_review_target_day_offset}"
        )

    def stop(self):
        """åœæ­¢å®šæ—¶ä»»åŠ¡"""
        if self.scheduler.running:
            self.scheduler.shutdown()
            logger.info("å®šæ—¶ä»»åŠ¡å·²åœæ­¢")

    def get_next_run_time(self):
        """è·å–ä¸‹æ¬¡è¿è¡Œæ—¶é—´"""
        job = self.scheduler.get_job('sync_trades_incremental')
        if not job:
            job = self.scheduler.get_job('sync_trades_full_daily')
        if job:
            return job.next_run_time
        return None


# å…¨å±€å®ä¾‹
scheduler_instance = None


def _env_is_truthy(value: str | None) -> bool:
    return str(value or "").strip().lower() in ("1", "true", "yes", "on")


def _resolve_worker_count() -> int:
    raw = os.getenv("WEB_CONCURRENCY") or os.getenv("UVICORN_WORKERS") or "1"
    try:
        count = int(raw)
    except (TypeError, ValueError):
        count = 1
    return max(1, count)


def should_start_scheduler() -> tuple[bool, str]:
    api_key = os.getenv("BINANCE_API_KEY")
    api_secret = os.getenv("BINANCE_API_SECRET")
    if not api_key or not api_secret:
        return False, "missing_api_keys"

    worker_count = _resolve_worker_count()
    allow_multi_worker = _env_is_truthy(os.getenv("SCHEDULER_ALLOW_MULTI_WORKER"))
    if worker_count > 1 and not allow_multi_worker:
        return False, "multi_worker_unsupported"

    return True, "ok"


def get_scheduler() -> TradeDataScheduler:
    """è·å–è°ƒåº¦å™¨å•ä¾‹"""
    global scheduler_instance
    if scheduler_instance is None:
        scheduler_instance = TradeDataScheduler()
    return scheduler_instance
